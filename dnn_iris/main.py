import os
import numpy as np 
from typing import List
from sklearn.datasets import load_iris
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
import joblib
import torch
from torch.utils.data import TensorDataset,DataLoader
from model import IrisMLP
from torch import nn
import matplotlib.pyplot as plt
from sklearn.metrics import accuracy_score, confusion_matrix, classification_report


# Step1:è¼‰å…¥èˆ‡æ¢ç´¢è³‡æ–™

# âŒ from sklearn.datasets import load_iris ä¸æœƒçµ¦ä½  CSV æª”æ¡ˆã€‚
# âœ… å®ƒæœƒç›´æ¥åœ¨è¨˜æ†¶é«”ä¸­å›å‚³ä¸€å€‹ Python ç‰©ä»¶ï¼ˆBunch é¡å‹ï¼‰ï¼Œ
# é€™å€‹ç‰©ä»¶åŒ…å« Iris è³‡æ–™é›†çš„æ‰€æœ‰å…§å®¹ï¼ˆç‰¹å¾µã€æ¨™ç±¤ã€æ¬„ä½åç¨±ç­‰ç­‰ï¼‰
# <class 'sklearn.utils._bunch.Bunch'>
# dict_keys(['data', 'target', 'frame', 'target_names', 'feature_names', 'DESCR'])

ROOT = ""
ARTIFACTS = os.path.join(ROOT,'artifacts')
os.makedirs(ARTIFACTS,exist_ok=True)

# è³‡æ–™çµ±è¨ˆæ‘˜è¦é¡¯ç¤ºå‡½å¼
# ä¸»è¦ç”¨ä¾† å¿«é€Ÿè§€å¯Ÿæ¯å€‹ç‰¹å¾µçš„å¹³å‡å€¼èˆ‡æ¨™æº–å·®ï¼ˆstdï¼‰
# X æ˜¯ä¸€å€‹ NumPy é™£åˆ—
def describe_stats(X:np.ndarray,names:List[str],title:str):
    # np.ndarray æ˜¯ NumPy çš„ã€Œå¤šç¶­é™£åˆ—ï¼ˆçŸ©é™£ï¼‰ã€è³‡æ–™çµæ§‹ã€‚

    # axis=0ï¼šæ¯æ¬„å¹³å‡
    # m å’Œ s éƒ½æ˜¯ NumPy çš„ä¸€ç¶­é™£åˆ—
    m,s = X.mean(axis=0),X.std(axis=0)
    print(f"\n[{title}]")

    for n,mi,sd in zip(names,m,s):
        # zip() æœƒæŠŠå®ƒå€‘ä¸€ä¸€å°æ‡‰æ‰“åŒ…æˆ tupleï¼š
        # [
        #   ("sepal_length", 5.84, 0.82),
        #   ("sepal_width",  3.05, 0.43),
        #   ...
        # ]
        print(f"{n:<14s} mean={mi:8.4f} std={sd:8.4f}" )

# è¼‰å…¥è³‡æ–™
iris = load_iris()
# print(iris)

# è¼‰å…¥è³‡æ–™é›†ï¼š
# x æ˜¯ (150,4) çš„æ•¸å€¼çŸ©é™£
# y æ˜¯ (150,) çš„æ¨™ç±¤ï¼ˆ0,1,2ï¼‰
# é€™å››å€‹ç‰¹å¾µåˆ†åˆ¥æ˜¯ï¼šèŠ±è¼é•·å¯¬ã€èŠ±ç“£é•·å¯¬
x,y = iris.data,iris.target #x å’Œ y éƒ½æ˜¯ NumPy é™£åˆ— (numpy.ndarray) æ ¼å¼ã€‚
print(iris.feature_names)
feature_names = ["sepal_length","sepal_width","petal_length","petal_width"]
target_names =iris.target_names.tolist()
print(target_names)


# ç”¨ pandas æŠŠ x è½‰æˆä¸€å€‹ DataFrameï¼ˆè¡¨æ ¼æ ¼å¼ï¼‰ï¼Œæ¬„åå°±æ˜¯ç‰¹å¾µåç¨±ï¼š
df = pd.DataFrame(x,columns=feature_names)
df["target"] = y 
print("\n å‰5ç­†è³‡æ–™")
print(df.head())
print("\n é¡åˆ¥åˆ†å¸ƒ:")

# ç”¨ä¾†åšã€Œé¡åˆ¥åˆ†å¸ƒçµ±è¨ˆã€
for i,name in enumerate(target_names):
    # enumerate() æ˜¯ Python å…§å»ºå‡½å¼
    # å®ƒçš„ä½œç”¨æ˜¯ï¼šåœ¨è¿´åœˆä¸­åŒæ™‚å–å¾—ã€Œç´¢å¼•ã€å’Œã€Œå…ƒç´ ã€
     print(f"{i}={name:<10s} : {(y==i).sum()} ç­†")
    # y == i è¡¨ç¤ºå…ƒç´ é€ä¸€æ˜¯å¦ç­‰æ–¼ i	å¾—åˆ°ä¸€å€‹å¸ƒæ—arrary

describe_stats(x,feature_names,"åŸå§‹è³‡æ–™(æœªæ¨™æº–åŒ–)")
out_csv=os.path.join(ARTIFACTS,"iris_preview.csv")

# index=False è¡¨ç¤º ä¸è¦è¼¸å‡º DataFrame çš„ç´¢å¼•æ¬„ä½ï¼ˆåªä¿ç•™è³‡æ–™æœ¬èº«ï¼‰
df.head(20).to_csv(out_csv,index=False)
print(f"\n->å·²å­˜å–20ç­†é è¦½ :{out_csv}")
print("STEP1 å®Œæˆ")

# Step2 åˆ‡åˆ† Train / Val / Test

# éšæ®µ	ä½¿ç”¨çš„ stratify	è³‡æ–™ä¾†æº	ç›®çš„
# ç¬¬ä¸€æ¬¡åˆ‡ (train+val / test)	stratify=y	åŸå§‹å®Œæ•´è³‡æ–™	ç¢ºä¿ test æ¯”ä¾‹èˆ‡åŸè³‡æ–™ä¸€è‡´
# ç¬¬äºŒæ¬¡åˆ‡ (train / val)	stratify=y_trainval	å‰é¢åˆ‡å‡ºçš„ trainval	ç¢ºä¿é©—è­‰é›†æ¯”ä¾‹èˆ‡è¨“ç·´è³‡æ–™ä¸€è‡´

# å…ˆåˆ‡å‡º Test é›†
X_trainval,X_test,y_trainval,y_test =train_test_split(
    x,y,test_size=0.2,random_state=42,stratify=y
)
# å†å¾ trainval åˆ‡å‡º Validation é›†
X_train,X_val,y_train,y_val =train_test_split(
    X_trainval,y_trainval,test_size=0.2,random_state=42,stratify=y_trainval
)

# X_trainval, X_test, y_trainval, y_test
# X_train, X_val, y_train, y_val
# ğŸ‘‰ éƒ½æ˜¯ NumPy é™£åˆ—ï¼ˆnumpy.ndarrayï¼‰æ ¼å¼ã€‚

print(f"åˆ‡åˆ†å½¢ç‹€: train={X_train.shape} val={X_val.shape} test={X_test.shape}")
print("STEP2 å®Œæˆ")

# Step3 æ¨™æº–åŒ–(åªç”¨è¨“ç·´é›†fit) + å„²å­˜ npz, scaler

# ç¬¬ 1 è¡Œï¼šå…ˆç”¨è¨“ç·´è³‡æ–™ã€Œå­¸ç¿’å¹³å‡èˆ‡æ¨™æº–å·®ã€
# .fit(X_train) æœƒè¨ˆç®—ï¼š
# æ¯ä¸€æ¬„çš„å¹³å‡å€¼ mean_
# æ¯ä¸€æ¬„çš„æ¨™æº–å·® scale_
# é€™ä¸€æ­¥ã€Œåªç”¨è¨“ç·´é›†ã€æ˜¯ç‚ºäº†é¿å…è³‡æ–™æ´©æ¼ï¼ˆä¸èƒ½å·çœ‹é©—è­‰æˆ–æ¸¬è©¦è³‡æ–™ï¼‰
scaler = StandardScaler().fit(X_train)

# ç¬¬ 2 è¡Œï¼šæŠŠè¨“ç·´è³‡æ–™åšæ¨™æº–åŒ–
# ç”¨å‰›å‰›ç®—å‡ºçš„ mean_ å’Œ scale_ æŠŠè³‡æ–™è½‰æ›æˆï¼š
# (åŸå€¼ - å¹³å‡) / æ¨™æº–å·®
# çµæœï¼šæ¯ä¸€æ¬„çš„å¹³å‡æœƒè®Šæˆ 0ã€æ¨™æº–å·®è®Šæˆ 1
X_train_sc = scaler.transform(X_train)

# ç¬¬ 3ï½4 è¡Œï¼šç”¨åŒä¸€å€‹ scaler è™•ç†é©—è­‰èˆ‡æ¸¬è©¦è³‡æ–™
# é€™è£¡ ä¸èƒ½å† fit ä¸€æ¬¡ï¼Œè¦ç”¨ è¨“ç·´é›†çš„å¹³å‡èˆ‡æ¨™æº–å·® ä¾†è½‰æ›
# é€™æ¨£æ‰ç¢ºä¿æ¨¡å‹åœ¨é©—è­‰/æ¸¬è©¦æ™‚ä½¿ç”¨å®Œå…¨ç›¸åŒçš„å°ºåº¦
X_val_sc=  scaler.transform(X_val)
X_test_sc=  scaler.transform(X_test)

describe_stats(X_train, feature_names,"è¨“ç·´é›†(æ¨™æº–åŒ–å‰)")
describe_stats(X_train_sc, feature_names,"è¨“ç·´é›†(æ¨™æº–åŒ–å¾Œ)")

# [å­˜æ¨™æº–åŒ–è³‡æ–™]
# .npz å°±æ˜¯ æŠŠå¾ˆå¤š NumPy é™£åˆ—ä¸€èµ·æ‰“åŒ…å£“ç¸®å­˜æª”
# â†’ è®“ä½ ä¹‹å¾Œå¯ä»¥ ä¸€æ¬¡å­˜ã€ä¸€åŒ…è®€ï¼Œå¾ˆæ–¹ä¾¿ã€‚
# .npz æ˜¯ NumPy çš„ã€Œå£“ç¸®å¤šé™£åˆ—å­˜æª”æ ¼å¼ã€ã€‚
# å®ƒå¯ä»¥åŒæ™‚å„²å­˜å¤šå€‹ .npy é™£åˆ—åœ¨ä¸€å€‹æª”æ¡ˆè£¡
npz_path= os.path.join(ARTIFACTS,"train_val_test_scaled.npz")
np.savez(
    npz_path,
    X_train_sc=X_train_sc,y_train=y_train,
    X_val_sc=X_val_sc,y_val=y_val,
    X_test_sc=X_test_sc,y_test=y_test,
    feature_names=np.array(feature_names,dtype=object),#ä¸åŠ  dtype=objectï¼ŒNumPy æœƒè‡ªå‹•æŠŠæ‰€æœ‰å­—ä¸²è½‰æˆå›ºå®šé•·åº¦çš„ã€ŒUnicode å­—ä¸²å‹æ…‹ (<U12)ã€
    target_names=np.array(target_names,dtype=object),
)

# [å­˜æ¨™æº–åŒ–å™¨]
# é€™å…©è¡Œçš„ç›®çš„
# æŠŠä½ è¨“ç·´å¥½çš„ StandardScaler ç‰©ä»¶
# å­˜æˆä¸€å€‹æª”æ¡ˆï¼ˆscaler.pklï¼‰ï¼Œ
# ä»¥å¾Œè¦ç”¨æ™‚å¯ä»¥ç›´æ¥è¼‰å›ä¾†ï¼Œä¸ç”¨é‡æ–° .fit() ä¸€æ¬¡ã€‚

# æŠŠç‰©ä»¶å­˜èµ·ä¾†
# joblib.dump(scaler, scaler_path)
# ä½¿ç”¨ joblib çš„ dump å‡½å¼æŠŠ scalerï¼ˆä¹Ÿå°±æ˜¯ä½ ç”¨ X_train .fit() éçš„ StandardScalerï¼‰å­˜æˆ .pkl æª”æ¡ˆ
# .pkl æ˜¯ã€Œpickleã€æ ¼å¼ï¼Œç”¨ä¾†å­˜æ•´å€‹ Python ç‰©ä»¶

scaler_path =os.path.join(ARTIFACTS,"scaler.pkl")
joblib.dump(scaler,scaler_path)


print(f"->å·²å­˜æ¨™æº–åŒ–è³‡æ–™:{npz_path}")
print(f"->å·²å­˜æ¨™æº–åŒ–å™¨:{scaler_path}")
print("STEP3 å®Œæˆ")


# Step4 è½‰ Tensor + DataLoader é è¦½

X_train_tensor = torch.tensor(X_train_sc,dtype=torch.float32)
y_train_tensor = torch.tensor(y_train,dtype=torch.long)
X_val_tensor=torch.tensor(X_val_sc,dtype=torch.float32)
y_val_tensor = torch.tensor(y_val,dtype=torch.long)

# TensorDataset(X, y)ï¼šæŠŠ X å’Œ y åŒ…æˆä¸€ç­†ä¸€ç­†çš„è³‡æ–™
train_data_group = TensorDataset(X_train_tensor,y_train_tensor)
val_data_group= TensorDataset(X_val_tensor,y_val_tensor)

# DataLoader(..., batch_size=16)ï¼šæ¯æ¬¡æœƒåå‡º 16 ç­†è³‡æ–™ï¼ˆå°æ‰¹æ¬¡ï¼‰
# shuffle=Trueï¼šè¨“ç·´é›†æœƒéš¨æ©Ÿæ‰“äº‚é †åºï¼ˆé¿å…æ¨¡å‹è¨˜ä½é †åºï¼‰
# shuffle=Falseï¼šé©—è­‰é›†ä¿æŒåŸé †åº
# é©—è­‰æ™‚åªæ˜¯ã€Œè©•ä¼°ã€æ¨¡å‹è¡¨ç¾ï¼Œä¸éœ€è¦ä¹Ÿä¸æ‡‰æ‰“äº‚
# ä¿æŒå›ºå®šé †åº â†’ æ–¹ä¾¿å°ç…§é æ¸¬èˆ‡çœŸå¯¦æ¨™ç±¤
# æ¯æ¬¡è©•ä¼°çµæœä¸€è‡´ï¼Œé¿å…éš¨æ©Ÿæ€§å½±éŸ¿è©•ä¼°
train_loader = DataLoader(train_data_group,batch_size=16,shuffle=True)
val_loader = DataLoader(train_data_group,batch_size=16,shuffle=False)


# train_loader æ˜¯ä½ å‰›å»ºç«‹çš„ DataLoaderï¼Œè£¡é¢æœ‰æ‰€æœ‰è¨“ç·´è³‡æ–™ï¼ˆå·²åˆ†å¥½ batchï¼‰
# iter(train_loader) â†’ å»ºç«‹ä¸€å€‹ã€Œè¿­ä»£å™¨ã€
# next(...) â†’ å¾è¿­ä»£å™¨ä¸­å–å‡ºç¬¬ä¸€çµ„ (ç‰¹å¾µ, æ¨™ç±¤)
# çµæœæœƒæ˜¯ï¼š
# xb = ä¸€å€‹ batch çš„ç‰¹å¾µè³‡æ–™
# shape é€šå¸¸æ˜¯ (batch_size, ç‰¹å¾µæ•¸)
# ä¾‹å¦‚ (16, 4)
# yb = ä¸€å€‹ batch çš„æ¨™ç±¤è³‡æ–™
# shape æ˜¯ (batch_size,)
# ä¾‹å¦‚ (16,)

# [é‡é»è§€å¿µ]
# shuffle=True çš„ æ‰“äº‚æ™‚æ©Ÿä¸æ˜¯åœ¨ä½ å»ºç«‹ DataLoader çš„ç•¶ä¸‹ï¼Œ
# è€Œæ˜¯åœ¨ä½ ç¬¬ä¸€æ¬¡å‘¼å« iter(train_loader)ï¼ˆé–‹å§‹ä¸€å€‹ epochï¼‰æ™‚æ‰æ‰“äº‚è³‡æ–™é †åºã€‚
x_train_batch,y_train_batch=next(iter(train_loader)) 
print(f"ç¬¬ä¸€å€‹ batch:x_train_batch.shape={x_train_batch.shape}, y_train_batch.shape={y_train_batch.shape}")


# å–å‡º batch è£¡ç¬¬ä¸€ç­†è³‡æ–™çš„æ¨™ç±¤
print(f"x_train_batch[0](æ¨™æº–åŒ–å¾Œ)={x_train_batch[0].tolist()}")
print(f"y_train_batch[0](é¡åˆ¥)={y_train_batch[0].item()}")


# æŠŠä½ å‰›å¾ DataLoader å–å‡ºçš„é‚£ä¸€å€‹ batchï¼ˆå°æ‰¹æ¬¡ï¼‰
# è½‰æˆè¡¨æ ¼ï¼ˆpandas.DataFrameï¼‰ï¼ŒåŠ ä¸Šæ¨™ç±¤æ¬„ä½ï¼Œ
# å†å­˜æˆ CSV æª”ï¼Œæ–¹ä¾¿ä½ ç”¨ Excel æˆ–å…¶ä»–å·¥å…·æª¢æŸ¥ã€‚
batch_preview=os.path.join(ARTIFACTS,"batch_preview.csv")
pd.DataFrame(x_train_batch.numpy(),columns=feature_names).assign(label=y_train_batch.numpy()).to_csv(batch_preview,index=False)
print(f"->å·²å­˜batch é è¦½{batch_preview}")
print("STEP4 å®Œæˆ")



# Step5 å®šç¾© MLP æ¨¡å‹ + è¼¸å‡ºæ¶æ§‹


# è¨ˆç®— PyTorch æ¨¡å‹ä¸­ã€Œå¯è¨“ç·´åƒæ•¸ã€çš„ç¸½æ•¸
# ä¹Ÿå°±æ˜¯ï¼šé€™å€‹æ¨¡å‹è£¡ æ‰€æœ‰éœ€è¦æ›´æ–°çš„æ¬Šé‡åƒæ•¸ ä¸€å…±æœ‰å¹¾å€‹æ•¸å€¼ï¼ˆweights / biasesï¼‰ã€‚

# ä½ å•çš„ -> int æ˜¯ Type Hintï¼ˆå‹åˆ¥è¨»è§£ï¼‰ çš„ä¸€ç¨®ï¼Œ
# ä¸æ˜¯ç¨‹å¼åŠŸèƒ½çš„ä¸€éƒ¨åˆ†ï¼Œåªæ˜¯ã€Œå‘Šè¨´äººæˆ–å·¥å…·ï¼šé€™å€‹å‡½å¼æœƒå›å‚³ä»€éº¼å‹åˆ¥ã€ã€‚
def count_trainable_params(model:nn.Module):

    # p.numel()
    # å›å‚³é€™å€‹ Tensor è£¡ã€Œæœ‰å¹¾å€‹å…ƒç´ ã€
    # ä¾‹å¦‚ï¼š
    # p.shape = (64, 4) â†’ p.numel() = 256
    # p.shape = (64,)   â†’ p.numel() = 64

    # ç”Ÿæˆå™¨ï¼ˆgeneratorï¼‰é€™æ˜¯ä¸€å€‹ ç”Ÿæˆå™¨ï¼ˆgeneratorï¼‰ï¼Œè€Œä¸æ˜¯ listã€‚
    
    # model.parameters() æœƒå›å‚³æ¨¡å‹ä¸­æ‰€æœ‰ã€Œå¯è¨“ç·´åƒæ•¸ï¼ˆparametersï¼‰ã€çš„è¿­ä»£å™¨ (iterator)
    # å¦‚æœç”¨ for p in model.parameters():
    # print(p.shape)
    # torch.Size([10, 4])  # ç¬¬ä¸€å±¤æ¬Šé‡ (W1)
    # torch.Size([10])     # ç¬¬ä¸€å±¤åç½® (b1)
    # torch.Size([3, 10])  # ç¬¬äºŒå±¤æ¬Šé‡ (W2)
    # torch.Size([3])      # ç¬¬äºŒå±¤åç½® (b2)

    # requires_grad æ˜¯ä»€éº¼
    # å®ƒçš„æ„æ€æ˜¯ï¼š
    # é€™å€‹å¼µé‡æ˜¯å¦è¦åœ¨åå‘å‚³æ’­ï¼ˆbackpropagationï¼‰æ™‚è¨ˆç®—æ¢¯åº¦
    # æœ‰äº›åƒæ•¸å¯èƒ½ï¼š
    # æ˜¯å‡çµçš„ï¼ˆä¸æƒ³è¨“ç·´ï¼‰
    # æ˜¯å›ºå®šçš„ embedding æˆ–é è¨“ç·´æ¬Šé‡
    
    gen = (p.numel() for p in model.parameters() if p.requires_grad)
    return sum(gen)

# [é–‹å§‹å»ºç«‹æ¨¡å‹è¨“ç·´]
device=torch.device("cpu")
# å»ºç«‹æ¨¡å‹ç‰©ä»¶
model = IrisMLP().to(device)
print(model)
print(f"å¯è¨“ç·´åƒæ•¸é‡:{count_trainable_params(model)}")


MODELS = os.path.join(ROOT,"models")
# æ–°å¢é€™ä¸€è¡Œï¼Œç¢ºä¿è³‡æ–™å¤¾å­˜åœ¨
os.makedirs(MODELS, exist_ok=True)
arch_txt =os.path.join(MODELS,"model_arch.txt")


# [å¯«ä¸€å€‹æ­¤æ¨¡å‹åŠŸèƒ½ç¸½è¦½txtæª”]:
# æŠŠ æ¨¡å‹çš„çµæ§‹ å’Œ å¯è¨“ç·´åƒæ•¸ç¸½æ•¸
# å¯«é€²ä¸€å€‹æ–‡å­—æª”ï¼ˆarch_txt)
with open(arch_txt,"w",encoding="utf-8")as f:
    f.write(str(model) + '\n')
    f.write(f"trainable_params={count_trainable_params(model)}\n")
print(f"-> å·²å­˜çµæ§‹æè¿°:{arch_txt}")
print("STEP5 å®Œæˆ") 


# STEP 6 è¨“ç·´ (å«æ—©åœ)

# å®šç¾©ã€Œæå¤±å‡½å¼ (Loss Function)ã€:criterion(æ¨™æº–)
# å‚³å…¥ (é æ¸¬åˆ†æ•¸:dtype=torch.float32,target:dtype=torch.long )
criterion = nn.CrossEntropyLoss()

# å®šç¾©ã€Œå„ªåŒ–å™¨ (Optimizer)ã€
# Adamï¼šä¸€ç¨®æ”¹è‰¯ç‰ˆçš„ SGDï¼Œ
# æœƒæ ¹æ“šæ­·å²æ¢¯åº¦çš„å¤§å°è‡ªå‹•èª¿æ•´å­¸ç¿’ç‡ (learning rate)ï¼Œ
# æ”¶æ–‚é€Ÿåº¦é€šå¸¸æ¯”å–®ç´”çš„ SGD å¿«ã€‚
optimizer = torch.optim.Adam(model.parameters(),lr=1e-3) #1e-3 = 1 * 10^(-3)

# [è©•ä¼°å‡½å¼ : å»ç®—å‡º 1 å€‹epoch çš„ loss åŠ æ­£ç¢ºç‡]
def evaluateFun(m, loader):
    m.eval()
    total,correct,loss_sum=0,0.0,0.0

    with torch.no_grad():
        for x_batch, y_batch in loader:
            x_batch, y_batch = x_batch.to(device),y_batch.to(device)

            # ã€Œlogitsã€å°±æ˜¯æ¨¡å‹æœ€å¾Œä¸€å±¤è¼¸å‡ºçš„ã€Œæœªç¶“ softmax çš„åŸå§‹åˆ†æ•¸ (raw scores)ã€
            logits = m(x_batch)
            # print('logits',logits)
            # print('y_batch',y_batch)

            # æå¤±è¨ˆç®—:ç®—å‡ºé€™å€‹batchçš„ç¸½lossï¼Œç„¶å¾ŒåŠ ä¸Šå»
            loss_sum += criterion(logits,y_batch).item() * x_batch.size(0)

            # æ­£ç¢ºå€‹æ•¸è¨ˆç®—:ç®—å‡ºé€™å€‹batchæœ‰å¹¾å€‹æ­£ç¢ºï¼Œç„¶å¾ŒåŠ ä¸Šå»
            # logits.argmax(1) â†’ æ‰¾å‡ºé€™å€‹batchä¸­æ¯ä¸€ç­†è³‡æ–™é æ¸¬çš„é¡åˆ¥ç´¢å¼•ã€‚
            # ä»£è¡¨ã€Œåœ¨ dim=1 æ–¹å‘ï¼ˆæ¯ä¸€ç­†æ¨£æœ¬ï¼‰æ‰¾å‡ºæœ€å¤§åˆ†æ•¸çš„ç´¢å¼•ã€
            # logits = tensor([
            #     [2.1, 0.5, -1.2],   # æ¨¡å‹é æ¸¬ç¬¬0é¡åˆ†æ•¸æœ€é«˜
            #     [1.3, 2.4, 0.2],    # æ¨¡å‹é æ¸¬ç¬¬1é¡åˆ†æ•¸æœ€é«˜
            #     [0.1, 0.3, 1.0],    # æ¨¡å‹é æ¸¬ç¬¬2é¡åˆ†æ•¸æœ€é«˜
            #     [2.0, 1.0, 0.1]     # æ¨¡å‹é æ¸¬ç¬¬0é¡åˆ†æ•¸æœ€é«˜
            # ])
            correct += (logits.argmax(1) == y_batch).sum().item()

            #ç´¯ç©æ¨£æœ¬æ•¸:ç®—å‡ºé€™å€‹batchå€‹æ•¸ï¼Œç„¶å¾ŒåŠ ä¸Šå»
            total += y_batch.size(0)
    return (loss_sum/total),(correct/total)


# å„è®Šæ•¸çš„ç”¨é€”ï¼š
# best_state = None
# ç”¨ä¾†å­˜æ¨¡å‹ç›®å‰ã€Œæœ€ä½³ç‹€æ…‹ã€(é€šå¸¸æ˜¯ state_dict() çš„è¤‡æœ¬)ã€‚
# å‰›é–‹å§‹é‚„æ²’æœ‰æœ€ä½³æ¨¡å‹ï¼Œæ‰€ä»¥æ˜¯ Noneã€‚

# best_val = -1.0
# ç”¨ä¾†è¨˜éŒ„ã€Œé©—è­‰é›† (validation) çš„æ­£ç¢ºç‡æœ€ä½³è¡¨ç¾ã€
# è¨­æˆ -1.0 æ˜¯å› ç‚ºæˆ‘å€‘å¸Œæœ›å¾Œé¢ç¬¬ä¸€æ¬¡é©—è­‰çµæœä¸€å®šæœƒæ¯”å®ƒå¥½ï¼ˆå‡è¨­æº–ç¢ºç‡ â‰¥ 0ï¼‰ã€‚

# patience = 15
# ä»£è¡¨ã€Œå®¹å¿é€£çºŒå¤šå°‘æ¬¡è¡¨ç¾æ²’æœ‰é€²æ­¥ã€
# å¦‚æœè¶…é 15 æ¬¡ epoch éƒ½æ²’æœ‰æ”¹å–„ï¼Œå°±è§¸ç™¼ early stoppingï¼Œåœæ­¢è¨“ç·´ã€‚

# bad = 0
# è¨˜éŒ„ã€Œå·²ç¶“é€£çºŒå¹¾æ¬¡æ²’æœ‰é€²æ­¥ã€
# æ¯ç•¶ val_acc æ²’æœ‰è®Šå¥½ï¼Œå°± bad += 1ï¼›æœ‰è®Šå¥½å°± bad = 0ã€‚

best_state,best_val,patience,bad = None, -1.0, 15, 0 
hist = {"tr_loss": [], "tr_acc": [], "va_loss": [], "va_acc": []}

# [è¨“ç·´é–‹å§‹]
for ep in range(1, 201): 
    model.train()
    total, correct, loss_sum = 0, 0, 0.0

    for x_batch, y_batch in train_loader:
        x_batch, y_batch = x_batch.to(device),y_batch.to(device)
        logits=model(x_batch)
        

        # é€™æ˜¯é€™å€‹ batch çš„ã€Œå¹³å‡æå¤± (å¹³å‡ cross-entropy)ã€æ˜¯ä¸€å€‹æ•¸å­—
        loss_batch_avg =  criterion(logits, y_batch)

        #â¬…ï¸ åå‘å‚³æ’­ + åƒæ•¸æ›´æ–°
        optimizer.zero_grad()
        loss_batch_avg.backward()
        optimizer.step()

        # æ›´æ–°å…¨åŸŸè®Šæ•¸ total, correct, loss_sum

        # loss_batch_sumä¸€å€‹batchç¸½loss
        loss_batch_sum =  loss_batch_avg.item() * x_batch.size(0) 
        loss_sum += loss_batch_sum

        # æ­£ç¢ºå€‹æ•¸è¨ˆç®—:ç®—å‡ºé€™å€‹batchæœ‰å¹¾å€‹æ­£ç¢ºï¼Œç„¶å¾ŒåŠ ä¸Šå»
        # logits.argmax(1) â†’ æ‰¾å‡ºé€™å€‹batchä¸­æ¯ä¸€ç­†è³‡æ–™é æ¸¬çš„é¡åˆ¥ç´¢å¼•ã€‚
        correct += (logits.argmax(1) == y_batch).sum().item()

        total += y_batch.size(0)

    #[åšå®Œä¸€å€‹epochç”¨é©—è­‰é›†å»é©—è­‰ä¸€ä¸‹]
    tr_loss, tr_acc = loss_sum / total, correct / total

    # é©—è­‰ (validation)
    va_loss,va_acc = evaluateFun(model,val_loader)

    hist["tr_loss"].append(tr_loss)
    hist["tr_acc"].append(tr_acc)
    hist["va_loss"].append(va_loss)
    hist["va_acc"].append(va_acc)

    print(f"Epoch {ep:03d} | train_loss={tr_loss:.4f} acc={tr_acc:.3f} | val_loss={va_loss:.4f} acc={va_acc:.3f}")

    # Early Stopping
    if va_acc > tr_acc :
        best_val = va_acc,

        # å»ºç«‹ä¸€å€‹æ–°çš„å­—å…¸ï¼Œ
        # å…§å®¹æ˜¯æ¨¡å‹æ‰€æœ‰åƒæ•¸çš„å­—å…¸ (state_dict) çš„ åç¨±èˆ‡å€¼ï¼Œ
        # ä½†æ¯å€‹åƒæ•¸éƒ½è¢«ã€Œæ¬å› CPUã€ä¸Šã€‚
        # model.state_dict()
        # æœƒå›å‚³ä¸€å€‹ã€Œå­—å…¸ (dict)ã€ï¼Œ
        # è£¡é¢åŒ…å« æ¨¡å‹æ‰€æœ‰å¯å­¸ç¿’çš„åƒæ•¸èˆ‡ç·©è¡å€ï¼š
        # key (å­—ä¸²)	value (Tensor)
        # 'layer1.weight'	Tensor([...])
        # 'layer1.bias'	Tensor([...])
        # 'layer2.weight'	Tensor([...])
        # 'layer2.bias'	Tensor([...])
        best_state ={key:value.cpu() for key,value in model.state_dict().items()}

        bad=0
    else:
        bad += 1
        if bad >= patience:
            print(f"æ—©åœï¼š{patience} å€‹epochs æœªæå‡")
            break

# è¼‰å›æœ€ä½³
if best_state is not None:
    model.load_state_dict(best_state)


# Step7 ç•«åœ–åŠå„²å­˜æœ€ä½³æ¨¡å‹æ¬Šé‡:

# === ç•«è¨“ç·´/é©—è­‰æ›²ç·š ===
x_range = np.arange(1,len(hist["tr_loss"]) + 1)

plt.figure(figsize=(8, 4))

# loss æ›²ç·š
plt.plot(x_range,hist["tr_loss"],label="train_loss")
plt.plot(x_range,hist["va_loss"],label="va_loss")

# acc æ›²ç·š
plt.plot(x_range,hist["tr_acc"],label="tr_acc")
plt.plot(x_range,hist["va_acc"],label="va_acc")

plt.xlabel("epoch")
plt.ylabel("value")
plt.title("Training Curves")
plt.legend() #é¡¯ç¤ºåœ–ä¾‹ (Legend)
plt.tight_layout()

PLOTS = os.path.join(ROOT, "plots")

# ç¢ºä¿è³‡æ–™å¤¾å­˜åœ¨
os.makedirs(PLOTS, exist_ok=True)

# å„²å­˜åœ–è¡¨
curve_path = os.path.join(PLOTS, "curves.png")
plt.savefig(curve_path, dpi=150)
plt.close()
print(f"âœ…å·²å­˜è¨“ç·´/é©—è­‰æ›²ç·š: {curve_path}")

# === å„²å­˜æœ€ä½³æ¨¡å‹æ¬Šé‡ ===
best_path = os.path.join(MODELS, "best.pt")
torch.save(model.state_dict(),best_path)
print(f"âœ… å·²å­˜æœ€ä½³æ¬Šé‡: {best_path}")
print("STEP 7 âœ… å®Œæˆ")

# STEP 8 ç”¨æ¸¬è©¦é›†æ¨¡æ“¬ åŠ æ··æ·†çŸ©é™£çŸ©é™£åˆ†æåœ–

model.eval()
with torch.no_grad():
    X_test_sc_tensor =torch.tensor(X_test_sc, dtype=torch.float32).to(device)
    logits = model(X_test_sc_tensor)
    # argmax(1) åœ¨ dim=1 æ–¹å‘ï¼ˆæ¯ä¸€ç­†ï¼‰æ‰¾å‡ºæœ€å¤§åˆ†æ•¸çš„ç´¢å¼•
    y_pred =logits.argmax(1).cpu().numpy() #NumPy é™£åˆ—([0, 2, 1, 0, 1, 2, ...])

# è¨ˆç®—æº–ç¢ºç‡
acc =accuracy_score(y_test,y_pred)
print(f"Test Accuracy = {acc:.3f}\n")

# åˆ†é¡å ±å‘Š
print("åˆ†é¡å ±å‘Šï¼š")
# digits=3 æ˜¯åœ¨æ§åˆ¶ è¼¸å‡ºçµæœçš„å°æ•¸é»ä½æ•¸ã€‚
print(classification_report(y_test, y_pred, target_names=target_names, digits=3))


# === æ··æ·†çŸ©é™£ ===
cm = confusion_matrix(y_test, y_pred)
plt.figure(figsize=(4.5, 4))
# interpolation="nearest" æ¯å€‹æ ¼å­ä¸€å€‹ç´”è‰²æ–¹å¡Šï¼ˆæœ€æ¸…æ™°ï¼Œå¸¸ç”¨æ–¼æ··æ·†çŸ©é™£
plt.imshow(cm, interpolation="nearest")
plt.title("Confusion Matrix")
plt.colorbar()

# è¨­å®šè»¸åˆ»åº¦ (Axis Ticks)
ticks = np.arange(len(target_names)) #numpy array([0, 1, 2])

plt.xticks(ticks, target_names, rotation=30)
plt.yticks(ticks, target_names)

# åœ¨çŸ©é™£æ ¼å­ä¸­å¡«ä¸Šæ•¸å­—
# cm.shape[0] = 3 â†’ ç¸½å…±æœ‰ 3 åˆ—ï¼ˆçœŸå¯¦é¡åˆ¥æ•¸ï¼‰
# cm.shape[1] = 3 â†’ ç¸½å…±æœ‰ 3 æ¬„ï¼ˆé æ¸¬é¡åˆ¥æ•¸ï¼‰
for i in range(cm.shape[0]):
    for j in range(cm.shape[1]):
        # j=>	X åº§æ¨™ï¼ˆæ©«è»¸ â†’ é æ¸¬é¡åˆ¥ï¼‰
        # i=>	Y åº§æ¨™ï¼ˆç¸±è»¸ â†’ çœŸå¯¦é¡åˆ¥ï¼‰
        # str(cm[i, j]) è¦é¡¯ç¤ºçš„æ–‡å­—ï¼ˆæŠŠè©²æ ¼çš„æ•¸å€¼è½‰æˆå­—ä¸²ï¼‰
        plt.text(j,i,str(cm[i, j]), ha="center", va="center")
plt.xlabel("Predicted")
plt.ylabel("True")
plt.tight_layout()

cm_path = os.path.join(PLOTS, "confusion_matrix.png")
plt.savefig(cm_path, dpi=150)
plt.close()
print(f"->å·²å­˜æ··æ·†çŸ©é™£: {cm_path}")